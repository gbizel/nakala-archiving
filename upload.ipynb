{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "- remove EXIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Notebook\n",
    "Uploads all files to Nakala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install required packages\n",
    "\n",
    "# create requirements.txt\n",
    "! echo \"pandas==1.1.4\" > requirements_upload.txt\n",
    "! echo \"requests==2.27.1\" >> requirements_upload.txt\n",
    "\n",
    "# install requirements\n",
    "! pip install -r requirements_upload.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, ast\n",
    "import csv, requests, json, time\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('./nakalapyconnect/'))\n",
    "import nklPushCorpus as npc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_csv('photos.csv')\n",
    "\n",
    "n_photos = len(data_pd)\n",
    "n_already_uploaded = sum(data_pd.Uploaded)\n",
    "n_to_upload = n_photos - n_already_uploaded\n",
    "\n",
    "print(\"{} photos have been identified: \".format(n_photos))\n",
    "print(\"- {} photos have already been uploaded\".format(n_already_uploaded))\n",
    "print(\"- {} photos need to be uploaded\".format(n_to_upload))\n",
    "\n",
    "# Endpoint & Clef d'API\n",
    "apiUrl = npc.myConst.API_URL\n",
    "apiKey = npc.API_KEY_NKL\n",
    "\n",
    "print('\\nKEYS:')\n",
    "print('URL:',apiUrl)\n",
    "print('KEY:',apiKey,'\\n')\n",
    "\n",
    "if apiUrl == 'https://apitest.nakala.fr':\n",
    "    is_api_test = True\n",
    "    print('You are working on the TEST API.')\n",
    "else:\n",
    "    is_api_test = False\n",
    "    print(\"/!\\ YOU ARE WORKING ON THE MAIN API, ANY CHANGE IS PERMANENT!\")\n",
    "    \n",
    "if is_api_test:\n",
    "    link_base = \"https://test.nakala.fr/\"\n",
    "else:\n",
    "    link_base = \"https://nakala.fr/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload collection\n",
    "\n",
    "### 1.1. Check if already uploaded collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection file\n",
    "list_collections = list(set(','.join(list(data_pd['Collections'])).split(',')))\n",
    "list_collections\n",
    "\n",
    "collections = pd.DataFrame(index=list_collections, columns = ['uploaded', 'link', 'error', 'date']).reset_index()\n",
    "collections = collections.rename({'index': 'collection'},axis=1)\n",
    "collections['uploaded']=0\n",
    "collections = collections.sort_values(by='collection').reset_index(drop=True)\n",
    "\n",
    "collections = collections#.iloc[0:1]\n",
    "print(\"{} collections have been found in the photos file\".format(len(collections)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(','.join(list(data_pd['Collections'])).split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find an existing version of collections.csv\n",
    "\n",
    "name_old_collection_file = 'collections.csv'\n",
    "\n",
    "try:\n",
    "    collections_old = pd.read_csv(name_old_collection_file)\n",
    "    print(\"an existing collecion file has been found, containing {} collections\".format(len(collections_old)))\n",
    "except:\n",
    "    collections_old = pd.DataFrame()\n",
    "    print(\"/!\\ no existing collecion file named '{}' has been found!\".format(name_old_collection_file))\n",
    "    print(\"If you already uploaded collections, make sure the file hasn't been moved or renamed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate new collections\n",
    "\n",
    "if len(collections_old) > 0:\n",
    "    merged = pd.merge(collections_old[\"collection\"], collections, on='collection', how='outer', indicator=True)\n",
    "    collections_new = merged[merged._merge == 'left_only']\n",
    "    collections_new = collections_new.drop(\"_merge\", axis=1)\n",
    "    \n",
    "else:\n",
    "    collections_new = collections\n",
    "\n",
    "print(\"{} new collections have been found\".format(len(collections_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new collections \n",
    "collections_updated = collections_old.append(collections_new)\n",
    "\n",
    "print(\"The new file contains {} new collections, including {} existing collections and {} new collections\"\\\n",
    "      .format(collections_updated.shape[0],\n",
    "              collections_old.shape[0],\n",
    "              collections_new.shape[0]))\n",
    "\n",
    "print(\"{} collections have already been uploaded\".format(sum(collections_updated['uploaded'] == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = collections_updated.set_index('collection')\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.to_csv('collections.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Upload collections not previously uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_collection_handles = {}\n",
    "\n",
    "for collection in collections.index:\n",
    "    \n",
    "    if collections.loc[collection, 'uploaded'] == 0:\n",
    "    \n",
    "        # création d'un dictionnaire dicoBody contenant le status, un nouveau title et la lang désirée\n",
    "        dicoBody = {}\n",
    "        dicoBody[\"status\"] = \"public\"\n",
    "        dicoBody[\"metas\"] = []\n",
    "        metaTitle={\"value\": collection,\n",
    "          \"lang\": \"fr\",\n",
    "          \"typeUri\": \"http://www.w3.org/2001/XMLSchema#string\",\n",
    "          \"propertyUri\": \"http://nakala.fr/terms#title\"\n",
    "          }\n",
    "        dicoBody[\"metas\"].append(metaTitle)\n",
    "\n",
    "        # appel de la fonction post_collection\n",
    "        response = npc.post_collections(dicoBody)\n",
    "\n",
    "        if response.status_code == 201:\n",
    "            response = json.loads(response.text)\n",
    "            collections.loc[collection, 'uploaded'] = 1\n",
    "            collections.loc[collection, 'link'] = link_base+'collection/'+response['payload']['id']\n",
    "            collections.loc[collection, 'date'] = str(datetime.now())[0:19]\n",
    "            print(\"Collection '{}' successfully uploaded: {}\".format(collection,\n",
    "                                                                     collections.loc[collection, 'link']))\n",
    "\n",
    "        else:\n",
    "            collections.loc[collection, 'error'] = str(response.text)\n",
    "            print(\"Collection {} not uploaded! EROR: {}\".format(collection,\n",
    "                                                                response))\n",
    "\n",
    "        collections.to_csv('collections.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_not_uploaded_collections = sum(collections.uploaded == 0)\n",
    "\n",
    "if n_not_uploaded_collections == 0:\n",
    "    print(\"All collections have been uploaded! Proceed to next step.\")\n",
    "else:\n",
    "    print(\"/!\\ {} collections have not been uploade! Rerun previous cell.\".format(n_not_uploaded_collections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather collection handles\n",
    "for collection in collections.index:\n",
    "    if collections.loc[collection, 'uploaded'] ==1:\n",
    "        dict_collection_handles[collection] = collections.loc[collection, 'link'].replace(link_base+'collection/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## UPLOAD DES DONNEES UNE PAR UNE:\n",
    "\n",
    "\n",
    "status = 'published'\n",
    "\n",
    "nakalaTypeDict = {\n",
    "    \"Image\": \"http://purl.org/coar/resource_type/c_c513\",\n",
    "}\n",
    "\n",
    "nakalaCollectionDict = dict_collection_handles\n",
    "\n",
    "# 1.1 Lecture du fichier CSV\n",
    "with open('photos.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    dataset = list(reader)\n",
    "dataset.pop(0) # suppression des titres des colonnes\n",
    "\n",
    "# 2. Parcours des différentes lignes du fichier\n",
    "for num, data in enumerate(dataset):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    title = data[1]\n",
    "    filenames = data[2].split(';')\n",
    "    description = data[3]\n",
    "    authors = list(filter(None,data[4].split(';')))\n",
    "    date = data[5]\n",
    "    keywords = list(filter(None,data[6].split(',')))\n",
    "    datatype = data[7]\n",
    "    license = data[8]\n",
    "    nakalaCollections = list(filter(None,data[9].split(',')))\n",
    "    is_uploaded = data[11]\n",
    "        \n",
    "    if is_uploaded == '0': # on n'upload que si pas déjà uploadé\n",
    "        \n",
    "        if set(nakalaCollections).issubset(dict_collection_handles.keys()):\n",
    "\n",
    "            # 2.1. Récupération des infos disponibles sur la donnée à créer\n",
    "\n",
    "            print('CREATION DE LA DONNEE ' + str(num) + \" : \" + title)\n",
    "\n",
    "            # 2.2. Envoi des fichiers à l'API\n",
    "            files = [] # variable pour stocker les informations retournées en JSON par l'API à chaque upload.\n",
    "            for filename in filenames: # on parcours l'ensemble des fichiers d'une donnée\n",
    "\n",
    "\n",
    "                #### INSERT: replace new filename with EXIF / Compression (temp compressed)\n",
    "                # ============================\n",
    "\n",
    "\n",
    "                print('Envoi du fichier ' + filename + '...') # on affiche un message pour le suivi de l'upload\n",
    "                goToNextData = False\n",
    "                # écriture de la requête à l'API (ne contient pas de body en JSON, mais un fichier et un clef d'API)\n",
    "                payload={}\n",
    "                postfiles=[('file',(filename,open(filename, 'rb')))]\n",
    "                headers = {'X-API-KEY': apiKey }\n",
    "                # appel à l'API pour uploader le fichier\n",
    "                response = requests.request(\"POST\", apiUrl + '/datas/uploads', headers=headers, data=payload, files=postfiles)\n",
    "                # si l'upload s'est bien passé, on stocke les informations retournés par l'API dans la variable 'files'\n",
    "                if ( 201 == response.status_code ):\n",
    "                    # avant de stocker les informations retournées par l'API sur le fichier, on y ajoute une date d'embargo\n",
    "                    file = json.loads(response.text)\n",
    "                    file[\"embargoed\"] = time.strftime(\"%Y-%m-%d\") # on renseigne la date du jour si pas de date d'embargo renseignée\n",
    "                    files.append(file)\n",
    "                else:\n",
    "                    # une erreur s'est produite avec un upload\n",
    "                    print (\"Certains fichiers n'ont pas pu être envoyés, on passe à la donnée suivante...\") # on affiche un message d'erreur\n",
    "                    goToNextData = True # on stocke dans cette variable qu'il y a eu un problème\n",
    "                    break # on arrête l'upload des autres fichiers de la donnée\n",
    "            if goToNextData: continue # on passe à la donnée suivante si\n",
    "\n",
    "            # 2.4. Reconstruction des métadonnées\n",
    "            metas = [] # on stocke dans cette variable l'ensemble des métadonnées dans le format attendu\n",
    "\n",
    "            # la métadonnée type (obligatoire)\n",
    "            metaType = {\n",
    "                \"value\": nakalaTypeDict[datatype], # on insère ici le contenu de la colonne \"datatype\" mappé sur l'URI correspondante (un seul type par donnnée)\n",
    "                \"typeUri\": \"http://www.w3.org/2001/XMLSchema#anyURI\", # on indique ici que la valeur renseignée est de type URI\n",
    "                \"propertyUri\": \"http://nakala.fr/terms#type\" # on indique ici le champ \"type\" issu du vocabulaire NAKALA\n",
    "            }\n",
    "            metas.append(metaType) # ajout de la métadonnée dans le tableau \"metas\"\n",
    "\n",
    "            # la métadonnée titre (obligatoire)\n",
    "            metaTitle = {\n",
    "                \"value\": title, # on insère ici le contenu de la colonne \"nakala:title (fr)\" (un seul titre fr par donnnée)\n",
    "                \"lang\": \"fr\", # on indique ici la langue du titre (cf. ISO-639-1 ou ISO-639-3 pour les langues moins courantes)\n",
    "                \"typeUri\": \"http://www.w3.org/2001/XMLSchema#string\", # on indique ici que la valeur renseignée est une chaîne de caractères\n",
    "                \"propertyUri\": \"http://nakala.fr/terms#title\" # on indique ici le champ \"title\" issu du vocabulaire de NAKALA\n",
    "            }\n",
    "            metas.append(metaTitle)\n",
    "\n",
    "            # les métadonnées auteurs (obligatoire pour une donnée publiée)\n",
    "            if not authors: # si vide, on ajoute une métadonnée nakala:creator \"anonyme\"\n",
    "               metaAuthor = {\n",
    "                  \"value\": None, # sic. None sans guillemet veut dire \"null\" en Python\n",
    "                  \"propertyUri\": \"http://nakala.fr/terms#creator\" # on indique ici le champ \"creator\" issu du vocabulaire de NAKALA\n",
    "               }\n",
    "               metas.append(metaAuthor)\n",
    "            else:    \n",
    "                for author in authors: # la colonne \"nakala:creator\" peut comporter plusieurs valeurs qu'on parcours une à une\n",
    "                       # pour chaque valeur, on sépare le nom et le prénom pour construire la métadonnée nakala:creator\n",
    "                    surnameGivennameORCID = author.split(',')\n",
    "                    metaAuthor = {\n",
    "                        \"value\": { # la valeur de cette métadonnée n'est pas une simple chaîne de caractères, mais un objet composé d'au minimum deux propriétées (givenname et surname)\n",
    "                            \"givenname\": surnameGivennameORCID[1], # on insère ici le prénom\n",
    "                            \"surname\": surnameGivennameORCID[0] # on insère ici le nom de famille\n",
    "                        },\n",
    "                        \"propertyUri\": \"http://nakala.fr/terms#creator\" # on indique ici le champ \"creator\" issu du vocabulaire de NAKALA\n",
    "                    }\n",
    "                    # on ajoute le numéro ORCID (si présent)\n",
    "                    if len(surnameGivennameORCID) == 3:\n",
    "                        metaAuthor[\"value\"][\"orcid\"] = surnameGivennameORCID[2] # on insère ici le numéro ORCID\n",
    "                    # ajout de la métadonnée\n",
    "                    metas.append(metaAuthor)\n",
    "\n",
    "            # la métadonnée date de création (obligatoire pour une donnée publiée)\n",
    "            if not date: # si vide, on ajoute une métadonnée \"nakala:created\" \"inconnue\"\n",
    "                metaCreated = {\n",
    "                    \"value\": None,\n",
    "                    \"propertyUri\": \"http://nakala.fr/terms#created\"\n",
    "                }\n",
    "            else:\n",
    "                metaCreated = {\n",
    "                    \"value\": date, # on insère ici le contenu de la colonne \"nakala:created\" (une seule date de création par donnée)\n",
    "                    \"typeUri\": \"http://www.w3.org/2001/XMLSchema#string\",\n",
    "                    \"propertyUri\": \"http://nakala.fr/terms#created\"\n",
    "                }\n",
    "            metas.append(metaCreated)\n",
    "\n",
    "            # la métadonnée licence (obligatoire pour une donnée publiée)\n",
    "            metaLicense = {\n",
    "                \"value\": license, # On insère ici le contenu de la colonne \"nakala:license\" (une seule licence par donnée)\n",
    "                \"typeUri\": \"http://www.w3.org/2001/XMLSchema#string\",\n",
    "                \"propertyUri\": \"http://nakala.fr/terms#license\"\n",
    "            }\n",
    "            metas.append(metaLicense)\n",
    "\n",
    "            # la métadonnée description (facultative)\n",
    "            metaDescription = {\n",
    "                \"value\": description, # on insère ici le contenu de la colonne \"dcterms:description (fr)\" (une seule description par donnée)\n",
    "                \"lang\": \"fr\",\n",
    "                \"typeUri\": \"http://www.w3.org/2001/XMLSchema#string\",\n",
    "                \"propertyUri\": \"http://purl.org/dc/terms/description\" # notez qu'il ne s'agit plus ici d'une propriété issue du vocabulaire NAKALA, mais du vocabulaire Dcterms\n",
    "            }\n",
    "            metas.append(metaDescription)\n",
    "\n",
    "            # les métadonnées mots-clés (facultatives)\n",
    "            for keyword in keywords: # on parcours les valeurs de la colonne \"dcterms:subject (fr)\"\n",
    "                metaKeyword = {\n",
    "                    \"value\": keyword, # on insère ici la valeur d'un mot-clé\n",
    "                    \"lang\": \"fr\",\n",
    "                    \"typeUri\": \"http://www.w3.org/2001/XMLSchema#string\",\n",
    "                    \"propertyUri\": \"http://purl.org/dc/terms/subject\" # notez qu'il ne s'agit plus ici d'une propriété issue du vocabulaire NAKALA, mais du vocabulaire Dcterms\n",
    "                }\n",
    "                metas.append(metaKeyword)\n",
    "\n",
    "\n",
    "            # 2.6. Reconstruction des collections\n",
    "            collectionsIds = []\n",
    "            for nakalaCollection in nakalaCollections:\n",
    "                collectionsIds.append(nakalaCollectionDict[nakalaCollection])\n",
    "\n",
    "            # 3. Envoi de la donnée à NAKALA\n",
    "            postdata = { # variable contenant le contenu de la requête\n",
    "                \"status\" : status, # on publie directement les données\n",
    "                \"files\" : files,\n",
    "                \"metas\" : metas,\n",
    "                #\"rights\": rights,\n",
    "                \"collectionsIds\": collectionsIds\n",
    "            }\n",
    "            content = json.dumps(postdata) # serialisation du contenu en JSON\n",
    "            headers = { # header de la requête\n",
    "              'Content-Type': 'application/json',\n",
    "              'X-API-KEY': apiKey,\n",
    "            }\n",
    "            response = requests.request(\"POST\", apiUrl + '/datas', headers=headers, data=content) # requête à l'API\n",
    "            if ( 201 == response.status_code ): # on obtient un code 201 si tout s'est bien passé\n",
    "                parsed = json.loads(response.text) # on parse la réponse de l'API\n",
    "                print('La donnée ' + str(num) + ' a bien été créée : ' + link_base + parsed[\"payload\"][\"id\"]) # affichage d'un message de succès            \n",
    "\n",
    "                data_pd.iloc[num,11] = 1\n",
    "                data_pd.iloc[num,12] = str(datetime.now())[0:19]\n",
    "                data_pd.iloc[num,13] = link_base + parsed[\"payload\"][\"id\"]\n",
    "                data_pd.iloc[num,0]  = status\n",
    "                data_pd.iloc[num,14] = ''\n",
    "                data_pd.to_csv('photos.csv', index=False)\n",
    "                \n",
    "                t1 = time.time()\n",
    "                print(\"photos.csv successfully updated! Upload took: {:.2f} seconds\".format(t1-t0))\n",
    "                print('\\n')\n",
    "\n",
    "            else:\n",
    "                print(\"Une erreur {} s'est produite !\".format(response.status_code))\n",
    "                json_resp = json.loads(response.text)\n",
    "                print(json_resp)\n",
    "\n",
    "                data_pd.iloc[num,12] = str(datetime.now())[0:19]\n",
    "                data_pd.iloc[num,14] = str(response.text)\n",
    "                data_pd.to_csv('photos.csv', index=False)\n",
    "                t1 = time.time()\n",
    "                print(\"photos.csv uploaded with error. Failed upload took: {:.2f} seconds\".format(t1-t0))\n",
    "                print('\\n')\n",
    "\n",
    "        \n",
    "        else:\n",
    "            print(\"{} cannot be uploaded: missing one of {} keys\".format(title, nakalaCollections))\n",
    "\n",
    "    else:\n",
    "        print(\"{} already uploaded\".format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

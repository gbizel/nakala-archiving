{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "- add Year in keywords (will change the indexation of collections)\n",
    "- handle missing keywords if not uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive: Link metadata to photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install required packages\n",
    "\n",
    "# create requirements.txt\n",
    "! echo \"pandas==1.1.4\" > requirements.txt\n",
    "! echo \"numpy==1.19.4\" >> requirements.txt\n",
    "! echo \"tqdm==4.54.0\" >> requirements.txt\n",
    "#! echo \"xlrd==1.2.0\" >> requirements.txt\n",
    "\n",
    "# install requirements\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import required packages\n",
    "\n",
    "import os, ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Variables to be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to photos root\n",
    "path_photos = 'base photos/photos terrains'\n",
    "\n",
    "# path to cartes root\n",
    "path_cartes = 'base photos/cartes'\n",
    "\n",
    "# authors path\n",
    "path_authors = 'authors.xls'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creation of a dictionnary storing each folder code with the matching metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_sheets(path):\n",
    "    \"\"\"\n",
    "    creates a dataframe with all sheets concatenated from an excel file\n",
    "    \"\"\"\n",
    "    xl = pd.ExcelFile(path)\n",
    "    n_sheets = len(xl.sheet_names)\n",
    "    for i in range(0, n_sheets):\n",
    "        if i == 0:\n",
    "            df = xl.parse(i)\n",
    "        else:\n",
    "            df = df.append(xl.parse(i))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_cartes_info(path):\n",
    "    \"\"\"\n",
    "    creates a list of dataframes,\n",
    "    each dataframe containing the information of each sites for a specific type\n",
    "    \n",
    "    ignores all files that do not work\n",
    "    \"\"\"\n",
    "\n",
    "    cartes_info = []\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            df_file = read_all_sheets(path + '/' + file)\n",
    "            cartes_info += [df_file]\n",
    "        except Exception as e:\n",
    "            print('\\nFile discarded as a carte :', file)\n",
    "            print('Error :', e)\n",
    "            \n",
    "    return(cartes_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(string):\n",
    "    \"\"\"\n",
    "    removes all digits from a string\n",
    "    \"\"\"\n",
    "    return ''.join([i for i in string if not i.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_dictionary(list_df_files):\n",
    "    \"\"\"\n",
    "    returns a dictionary, the keys are the code types, the values are the associated dataframes\n",
    "    \"\"\"\n",
    "        \n",
    "    code_dictionary = {}\n",
    "\n",
    "    for df in list_df_files:\n",
    "\n",
    "        first_code = df['Code Folder'].iloc[0]\n",
    "        code_root = remove_digits(first_code)\n",
    "\n",
    "        code_dictionary[code_root] = df\n",
    "\n",
    "    return code_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartes_info = gather_cartes_info(path_cartes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dictionary = get_code_dictionary(cartes_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creation of a dataframe containing all photo names and their code folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_files(dir_name):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    list_of_files = os.listdir(dir_name)\n",
    "    all_files = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in list_of_files:\n",
    "        # Create full path\n",
    "        full_path = os.path.join(dir_name, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(full_path):\n",
    "            all_files = all_files + get_list_of_files(full_path)\n",
    "        else:\n",
    "            all_files.append(full_path)\n",
    "                \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_code_photos(photos):\n",
    "    \n",
    "    # Create empty dataframe\n",
    "    photos_info = pd.DataFrame(columns = ['photo_name','code_folder', 'path_folder'])\n",
    "    \n",
    "    \n",
    "    # add photo infos to dataframe\n",
    "    for i in tqdm(range(0,len(photos))):\n",
    "\n",
    "        # Exclude files with '#' in the path\n",
    "        if photos[i].find('#') == -1 :\n",
    "\n",
    "            split_path = photos[i].split('/')\n",
    "\n",
    "            photo = split_path[-1]\n",
    "\n",
    "            # Exclude non '.jpg' or '.JPG' files\n",
    "            if (photo.split('.')[-1] == 'JPG') | (photo.split('.')[-1] == 'jpg'):\n",
    "\n",
    "                folder_name = split_path[-2]\n",
    "                code_folder = folder_name.split(' ')[-1]\n",
    "                path_folder = '/'.join(photos[i].split('/')[:-1])\n",
    "\n",
    "                # add photo infos to dataframe\n",
    "                s = pd.Series([photo,code_folder, path_folder],index=['photo_name','code_folder', 'path_folder'])\n",
    "                photos_info = photos_info.append(s,ignore_index=True)\n",
    "\n",
    "    print(\"{} photos were found, {} were discarded because a '#' was found in the path\".format(i,i-photos_info.shape[0]))\n",
    "    print(\"dataframe contains {} photos before duplicate management\".format(photos_info.shape[0]))\n",
    "\n",
    "    # Aggregate photos in multiple folder (add '+' in code_folder)\n",
    "    photos_info = photos_info.groupby('photo_name').agg(\n",
    "                    {'code_folder':(lambda x: '+'.join(x)),\n",
    "                     'path_folder':(lambda x: list(x))}).reset_index()\n",
    "\n",
    "    print('dataframe contains {} photos after duplicate management'.format(photos_info.shape[0]))\n",
    "    \n",
    "    return photos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = get_list_of_files(path_photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info = match_code_photos(photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Match photos & site information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata(photos_info, code_dictionary):\n",
    "    \n",
    "    list_metadata_issues = []\n",
    "    photos_info['metadata'] = [[] for _ in range(len(photos_info))]\n",
    "\n",
    "\n",
    "    for i in tqdm(range(0,len(photos_info))):\n",
    "        code_folder = photos_info.iloc[i].code_folder\n",
    "\n",
    "        code_list = code_folder.split('+')\n",
    "\n",
    "        for code in code_list:    \n",
    "            code_type = remove_digits(code)\n",
    "            try :\n",
    "                df_code = code_dictionary[code_type]\n",
    "                row_code = df_code[df_code['Code Folder'] == code]\n",
    "                metadata = row_code[['Code Display',\n",
    "                                     'Name',\n",
    "                                     'Type',\n",
    "                                     'Latitude',\n",
    "                                     'Longitude',\n",
    "                                     'Location',\n",
    "                                     'Region']].values.tolist()[0]\n",
    "                photos_info.iloc[i,3] += [metadata]\n",
    "\n",
    "            except Exception as z:\n",
    "                print('metadata not added for line {} becasue error with: {}  // ({})'\\\n",
    "                      .format(i,\n",
    "                              z,\n",
    "                              photos_info.photo_name.iloc[i]))\n",
    "                list_metadata_issues += [photos_info.photo_name.iloc[i]]\n",
    "                \n",
    "                \n",
    "    n_issues = len(list_metadata_issues)\n",
    "    \n",
    "    \n",
    "    if n_issues > 0:\n",
    "        print('‚ùóÔ∏è‚ùóÔ∏è‚ö†Ô∏è A total of {} photos were skipped because of a metadata matching issue!'.format(n_issues))\n",
    "    else:\n",
    "        print('All pictures were matched with metadata successfully! üéâ')\n",
    "    photos_info = photos_info[~photos_info.photo_name.isin(list_metadata_issues)]\n",
    "\n",
    "    return photos_info, n_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence(metadata):\n",
    "    \n",
    "    n_codes = len(metadata)\n",
    "    \n",
    "    if n_codes == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        sites = \"\"\n",
    "\n",
    "        for i, metadata_i in enumerate(metadata):\n",
    "            \n",
    "            code, name, type_code, latitude, longitude, location, region = metadata_i\n",
    "            \n",
    "            #import pdb; pdb.set_trace()\n",
    "            \n",
    "            longitude = str(longitude)[0:8]\n",
    "            latitude = str(latitude)[0:8]\n",
    "            \n",
    "            site_i = f\"{name} (code: {code}, type: {type_code}, coordinates: {longitude}¬∞N {latitude}¬∞E)\"\n",
    "            \n",
    "            if i != n_codes - 1:\n",
    "                site_i += ', '\n",
    "            else:\n",
    "                site_i += '. '\n",
    "            \n",
    "            sites += site_i\n",
    "\n",
    "        if n_codes == 1:\n",
    "            intro = \"This is a picture of a heritage site in Ladakh. \"\n",
    "            intro_p2 = \"The site is: \"\n",
    "\n",
    "        if n_codes > 1:\n",
    "            intro = \"This is a picture of \" + str(n_codes) + \" heritage sites in Ladakh. \"\n",
    "            intro_p2 = \"The sites are: \"\n",
    "        \n",
    "        location_sentence = f\"Location: {location} ({region}). \"\n",
    "        \n",
    "        sentence = intro + location_sentence + intro_p2 + sites + \"More information: ladakharchaeology.com\"\n",
    "        \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentences(photos_info):\n",
    "    photos_info['sentence'] = photos_info.metadata.progress_apply(lambda x: sentence(x))\n",
    "    return photos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info, n_skipped = add_metadata(photos_info, code_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info = add_sentences(photos_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reshape file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_excel(path_authors)\n",
    "authors = authors[['Abreviation', 'Last Name','First Name', 'ORCID']]\n",
    "authors = authors.fillna('')\n",
    "\n",
    "dict_authors = {}\n",
    "for abreviation in authors.Abreviation:\n",
    "    row = authors[authors.Abreviation == abreviation]\n",
    "    if row.ORCID.iloc[0] == '':\n",
    "        dict_authors[abreviation] = ','.join(list(authors[authors.Abreviation == abreviation]\\\n",
    "                                                 [['Last Name', 'First Name']].iloc[0]))\n",
    "    else:\n",
    "        dict_authors[abreviation] = ','.join(list(authors[authors.Abreviation == abreviation]\\\n",
    "                                                 [['Last Name', 'First Name', 'ORCID']].iloc[0]))\n",
    "        \n",
    "dict_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#photos_info = pd.read_csv('photos_info.csv', index_col=0)\n",
    "#photos_info['path_folder'] = photos_info['path_folder'].apply(lambda x: ast.literal_eval(x))\n",
    "#photos_info['metadata'] = photos_info['metadata'].apply(lambda x: ast.literal_eval(x))\n",
    "photos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Status'] = 0 #'pending'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Title'] = photos_info['photo_name'].apply(lambda x: x.split('.')[0]) #.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Path'] = photos_info['path_folder'].apply(lambda x: x[0]) + '/' + photos_info['photo_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Description'] = photos_info['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_authors(photo_name):\n",
    "    try:\n",
    "        author = dict_authors[photo_name[0:2]]\n",
    "    except:\n",
    "        author = \"Devers,Quentin,0000-0001-8469-0165\"\n",
    "        print(\"/!\\ {} does not have a known author! QD was applied by default\".format(photo_name))\n",
    "    \n",
    "    return author "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Creator'] = photos_info['photo_name'].apply(lambda x: replace_authors(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Year'] = photos_info['photo_name'].apply(lambda x: x.split('.')[0].split('-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Type'] = 'Image' #'http://purl.org/coar/resource_type/c_c513'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['License'] = 'CC-BY-NC-4.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(row):\n",
    "\n",
    "    try:\n",
    "    \n",
    "        list_keywords = [' '.join(list(reversed(row['Creator'].split(',')[0:2]))), # creator\n",
    "                         \"Picture\",\n",
    "                         row.Year,\n",
    "                         row.metadata[0][6], # Region\n",
    "                         row.metadata[0][5], # Location\n",
    "                         row.metadata[0][2]  # Type\n",
    "                        ]\n",
    "\n",
    "        code_type = list(list(zip(*row.metadata))[0])\n",
    "        \n",
    "        return list_keywords + code_type\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error for photo: {} / type: {} / metadata: {} // Error: {}\".format(row.photo_name,\n",
    "                                                                                  row.code_folder,\n",
    "                                                                                  row.metadata,\n",
    "                                                                                  e)\n",
    "             )\n",
    "\n",
    "        return ['']*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info[photos_info.apply(lambda row: extract_keywords(row),axis=1).isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Keywords'] = photos_info.apply(lambda row: extract_keywords(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Collections'] = photos_info['Keywords'].apply(lambda x: ','.join(x[5:]))\n",
    "photos_info['Keywords'] = photos_info['Keywords'].apply(lambda x: ','.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Date scanned'] = str(datetime.now())[0:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info = photos_info.filter(['Status', 'Title', 'Path',\n",
    "                                  'Description', 'Creator', 'Year',\n",
    "                                  'Keywords', 'Type', 'License',\n",
    "                                  'Collections', 'Date scanned'])\n",
    "photos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#photos_info.to_csv('photos_info_nkl.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add columns to follow the upload process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info['Uploaded'] = 0\n",
    "photos_info['Upload date'] = 0\n",
    "photos_info['Link'] = ''\n",
    "photos_info['Error'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check if existing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find an existing version of photos.csv\n",
    "\n",
    "name_old_file = 'photos.csv'\n",
    "\n",
    "try:\n",
    "    photos_info_old = pd.read_csv(name_old_file)\n",
    "    print(\"an existing file has been found, containing {} photos\".format(len(photos_info_old)))\n",
    "except:\n",
    "    photos_info_old = pd.DataFrame()\n",
    "    print(\"/!\\ no existing file named '{}' has been found!\".format(name_old_file))\n",
    "    print(\"If you already uploaded photos, make sure the file hasn't been moved or renamed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate new photos\n",
    "\n",
    "if len(photos_info_old) > 0:\n",
    "    merged = pd.merge(photos_info_old[\"Title\"], photos_info, on='Title', how='outer', indicator=True)\n",
    "    photos_info_new = merged[merged._merge == 'right_only']\n",
    "    photos_info_new = photos_info_new.drop(\"_merge\", axis=1)\n",
    "    \n",
    "else:\n",
    "    photos_info_new = photos_info\n",
    "\n",
    "print(\"{} new photos have been found\".format(len(photos_info_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new photos \n",
    "photos_info_updated = photos_info_old.append(photos_info_new)\n",
    "\n",
    "print(\"The new file contains {} new photos:\\n- {} existing photos \\n- {} new photos\"\\\n",
    "      .format(photos_info_updated.shape[0],\n",
    "              photos_info_old.shape[0],\n",
    "              photos_info_new.shape[0]))\n",
    "\n",
    "if n_skipped >0:\n",
    "    print('‚ö†Ô∏è {} were skipped because of matching metadata issue'.format(n_skipped))\n",
    "\n",
    "print(\"{} photos have already been uploaded\".format(sum(photos_info_updated['Uploaded'] == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_info_updated.to_csv('photos.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive: Link metadata to photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.1.4 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.1.4)\n",
      "Requirement already satisfied: numpy==1.19.4 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.19.4)\n",
      "Requirement already satisfied: tqdm==4.54.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (4.54.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.1.4->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas==1.1.4->-r requirements.txt (line 1)) (2020.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.1.4->-r requirements.txt (line 1)) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "## install required packages\n",
    "\n",
    "# create requirements.txt\n",
    "! echo \"pandas==1.1.4\" > requirements.txt\n",
    "! echo \"numpy==1.19.4\" >> requirements.txt\n",
    "! echo \"tqdm==4.54.0\" >> requirements.txt\n",
    "\n",
    "# install requirements\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/std.py:699: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "## import required packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Variables to be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to photos root\n",
    "path_photos = 'base photos/photos terrains'\n",
    "\n",
    "# path to cartes root\n",
    "path_cartes = 'base photos/cartes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creation of a dictionnary storing each folder code with the matching metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_sheets(path):\n",
    "    \"\"\"\n",
    "    creates a dataframe with all sheets concatenated from an excel file\n",
    "    \"\"\"\n",
    "    xl = pd.ExcelFile(path)\n",
    "    n_sheets = len(xl.sheet_names)\n",
    "    for i in range(0, n_sheets):\n",
    "        if i == 0:\n",
    "            df = xl.parse(i)\n",
    "        else:\n",
    "            df = df.append(xl.parse(i))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_cartes_info(path):\n",
    "    \"\"\"\n",
    "    creates a list of dataframes,\n",
    "    each dataframe containing the information of each sites for a specific type\n",
    "    \n",
    "    ignores all files that do not work\n",
    "    \"\"\"\n",
    "\n",
    "    cartes_info = []\n",
    "    files = os.listdir(path)\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            df_file = read_all_sheets(path + '/' + file)\n",
    "            cartes_info += [df_file]\n",
    "        except Exception as e:\n",
    "            print('\\nFile discarded as a carte :', file)\n",
    "            print('Error :', e)\n",
    "            \n",
    "    return(cartes_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(string):\n",
    "    \"\"\"\n",
    "    removes all digits from a string\n",
    "    \"\"\"\n",
    "    return ''.join([i for i in string if not i.isdigit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_dictionary(list_df_files):\n",
    "    \"\"\"\n",
    "    returns a dictionary, the keys are the code types, the values are the associated dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    code_dictionary = {}\n",
    "\n",
    "    for df in list_df_files:\n",
    "\n",
    "        first_code = df['Code Folder'].iloc[0]\n",
    "        code_root = remove_digits(first_code)\n",
    "\n",
    "        code_dictionary[code_root] = df\n",
    "\n",
    "    return code_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File discarded as a carte : .DS_Store\n",
      "Error : Unsupported format, or corrupt file: Expected BOF record; found b'\\x00\\x00\\x00\\x01Bud1'\n",
      "\n",
      "File discarded as a carte : zz other\n",
      "Error : [Errno 21] Is a directory: 'base photos/cartes/zz other'\n",
      "\n",
      "File discarded as a carte : ~$p Rock art 11-2020 update 10-2021.xlsx\n",
      "Error : Unsupported format, or corrupt file: Expected BOF record; found b'\\x08quen ti'\n",
      "\n",
      "File discarded as a carte : ~$rc Regular chortens - 10oct21.xlsx\n",
      "Error : Unsupported format, or corrupt file: Expected BOF record; found b'\\x08quen ti'\n"
     ]
    }
   ],
   "source": [
    "cartes_info = gather_cartes_info(path_cartes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dictionary = get_code_dictionary(cartes_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['bc', 'ca', 'f', 'os', 'p', 'pc', 'rc', 'ss', 't'])\n"
     ]
    }
   ],
   "source": [
    "print(code_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creation of a dataframe containing all photo names and their code folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_files(dir_name):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    list_of_files = os.listdir(dir_name)\n",
    "    all_files = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in list_of_files:\n",
    "        # Create full path\n",
    "        full_path = os.path.join(dir_name, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(full_path):\n",
    "            all_files = all_files + get_list_of_files(full_path)\n",
    "        else:\n",
    "            all_files.append(full_path)\n",
    "                \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_code_photos(photos):\n",
    "    \n",
    "    # Create empty dataframe\n",
    "    photos_info = pd.DataFrame(columns = ['photo_name','code_folder', 'path_folder'])\n",
    "    \n",
    "    \n",
    "    # add photo infos to dataframe\n",
    "    for i in tqdm(range(0,len(photos))):\n",
    "\n",
    "        # Exclude files with '#' in the path\n",
    "        if photos[i].find('#') == -1 :\n",
    "\n",
    "            split_path = photos[i].split('/')\n",
    "\n",
    "            photo = split_path[-1]\n",
    "\n",
    "            # Exclude non '.jpg' or '.JPG' files\n",
    "            if (photo.split('.')[-1] == 'JPG') | (photo.split('.')[-1] == 'jpg'):\n",
    "\n",
    "                folder_name = split_path[-2]\n",
    "                code_folder = folder_name.split(' ')[-1]\n",
    "                path_folder = '/'.join(photos[i].split('/')[:-1])\n",
    "\n",
    "                # add photo infos to dataframe\n",
    "                s = pd.Series([photo,code_folder, path_folder],index=['photo_name','code_folder', 'path_folder'])\n",
    "                photos_info = photos_info.append(s,ignore_index=True)\n",
    "\n",
    "    print(\"{} photos were found, {} were discarded because a '#' was found in the path\".format(i,i-photos_info.shape[0]))\n",
    "    print(\"dataframe contains {} photos before duplicate management\".format(photos_info.shape[0]))\n",
    "\n",
    "    # Aggregate photos in multiple folder (add '+' in code_folder)\n",
    "    photos_info = photos_info.groupby('photo_name').agg(\n",
    "                    {'code_folder':(lambda x: '+'.join(x)),\n",
    "                     'path_folder':(lambda x: list(x))}).reset_index()\n",
    "\n",
    "    print('dataframe contains {} photos after duplicate management'.format(photos_info.shape[0]))\n",
    "    \n",
    "    return photos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = get_list_of_files(path_photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [00:00<00:00, 498.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393 photos were found, 48 were discarded because a '#' was found in the path\n",
      "dataframe contains 345 photos before duplicate management\n",
      "dataframe contains 342 photos after duplicate management\n"
     ]
    }
   ],
   "source": [
    "photos_info = match_code_photos(photos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Match photos & site information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata(photos_info, code_dictionary):\n",
    "    photos_info['metadata'] = [[] for _ in range(len(photos_info))]\n",
    "\n",
    "\n",
    "    for i in tqdm(range(0,len(photos_info))):\n",
    "        code_folder = photos_info.iloc[i].code_folder\n",
    "\n",
    "        code_list = code_folder.split('+')\n",
    "\n",
    "        for code in code_list:    \n",
    "            code_type = remove_digits(code)\n",
    "            try :\n",
    "                df_code = code_dictionary[code_type]\n",
    "                row_code = df_code[df_code['Code Folder'] == code]\n",
    "                metadata = row_code[['Code Display', 'Name', 'Type', 'Latitude', 'Longitude']].values.tolist()[0]\n",
    "                photos_info.iloc[i,3] += [metadata]\n",
    "\n",
    "            except Exception as z:\n",
    "                print('metadata not added for line ',i,'because error with: ', z)\n",
    "                \n",
    "    return photos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence(metadata):\n",
    "    \n",
    "    n_codes = len(metadata)\n",
    "    \n",
    "    if n_codes == 0:\n",
    "        return \"\"\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        sites = \"\"\n",
    "\n",
    "        #if len(code_sites) == 1: ### TODO more than 1 ###\n",
    "        for i, metadata_i in enumerate(metadata):\n",
    "\n",
    "            site_i = str(metadata_i[1]) \\\n",
    "                        + \" (code: \" + str(metadata_i[0]) \\\n",
    "                        + \", type: \" + str(metadata_i[2]) \\\n",
    "                        + \", coordinates : \" + str(metadata_i[3]) + \"°N \" \\\n",
    "                        + str(metadata_i[4]) + \"°E)\"\n",
    "            \n",
    "            if i != n_codes - 1:\n",
    "                site_i += ', '\n",
    "            else:\n",
    "                site_i += '. '\n",
    "            \n",
    "            sites += site_i\n",
    "\n",
    "        if n_codes == 1:\n",
    "            intro = \"This is a picture of a heritage site in Ladakh. The site is : \"\n",
    "\n",
    "        if n_codes > 1:\n",
    "            intro = \"This is a picture of \" + str(n_codes) + \" heritage sites in Ladakh. The sites are : \"\n",
    "        \n",
    "        sentence = intro + sites + \"More information: ladakharcheology.com\"\n",
    "        \n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentences(photos_info):\n",
    "    photos_info['sentence'] = photos_info.metadata.progress_apply(lambda x: sentence(x))\n",
    "    return photos_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [00:00<00:00, 611.18it/s]\n"
     ]
    }
   ],
   "source": [
    "photos_info = add_metadata(photos_info, code_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 342/342 [00:00<00:00, 37145.61it/s]\n"
     ]
    }
   ],
   "source": [
    "photos_info = add_sentences(photos_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4639434814453125\n"
     ]
    }
   ],
   "source": [
    "# End timer\n",
    "t1 = time() - t0\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save photos_info into a .csv file\n",
    "photos_info.to_csv('photos_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook archive.ipynb to script\n",
      "[NbConvertApp] Writing 7292 bytes to archive.py\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to script archive.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
